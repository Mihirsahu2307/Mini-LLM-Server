# Mini-LLM-Server
 Minimal server for local LLMs equipped with several inference optimization techniques. 
 
 Project is still under development. Few optimizations have been added, few more to come.

## TODO (features)
- Batching (static and dynamic)
- KV Caching
- Prompt Caching (Prefix Caching)
- Tensor Parallelism
- Speculative Decoding

## Papers
- ZeRO: https://arxiv.org/pdf/1910.02054
- Speculative Decoding: https://arxiv.org/pdf/2211.17192
- Self-Speculative Decoding: https://arxiv.org/pdf/2309.08168
- Survey blog: https://www.aussieai.com/research/inference-optimization
- Survey paper: https://arxiv.org/pdf/2404.14294 (pretty exhaustive)
